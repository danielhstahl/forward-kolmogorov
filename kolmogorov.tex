\documentclass{article}
\usepackage{amsmath}
\usepackage{fancyhdr}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{cite}
\usepackage{float}
\usepackage{scrextend}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{subcaption}

\setlength{\parindent}{0pt}
\newtheorem{theorem}{Theorem}
\theoremstyle{definition}
\newtheorem{definition}{Definition}

\begin{document}
\section{Backward and Forward Kolmogorov Equation}

This paper is focused on deriving both the Backward and Forward Kolmogorov equations.  These equations answer questions around how the density of a continuous stochastic differential equation (diffusion) evolves in both space and time.  This paper assumes an understanding of the theory of Brownian Motion and stochastic calculus (eg, existence and solutions to stochastic differential equations). For this paper, the stochastic differential equation under consideration is one in which a transition density exists and in which the dynamics are:

\begin{equation} \label{sde}
dX_t=\alpha(X_t) dt+\sigma(X_t) dW_t
\end{equation}

Where \(\alpha\) and \(\sigma\) satisfy the ``usual'' conditions and \(dW_t\) is an increment of Brownian Motion.  The ``differential'' form of this stochastic differential equation is shorthand for
\[X_T=x+\int_s^T \alpha(X_v) dv+\int_s^T\sigma(X_v) dW_v \]

Letting \(\mathcal{F}_t\) be the filtration generated by \(W_t\), note that (by uniqueness) that \(X_T\) conditional on \(\mathcal{F}_s\) is equal in distribution to:
\[X_{T-s}=x+\int_0^{T-s} \alpha(X_v) dv+\int_0^{T-s}\sigma(X_v) dW_v \]



\section{Evolution of Expectation of Stochastic Different Equations}

\begin{definition} \label{markov}
	A stochastic process \(Z_t\) is called \emph{Markov} if 
	\[\mathbb{E}[f(Z_T)|\mathcal{F}_s]=u(T-s, Z_s)\]
	In other words, a Markov process is one in which the expectation is a function only of current information and the expectation horizon.
\end{definition}

\begin{theorem}
	Let \(X_t\) be the solution to \ref{sde}.  Then \(X_t\) is Markov.  
\end{theorem}

\begin{proof}
	Stated here without proof; see standard textbooks such as Oksendal's Stochastic Differential Equations.  Note that definition \ref{markov} is more restrictive than the typical definition, and proving that \(X_t\) is Markov by this stricter definition also requires Dynkan's formula.
\end{proof}

\begin{definition}
	The \emph{generator} of the diffusion \(X_t\) is defined as 
	\[\lim_{T \to 0} \frac{\mathbb{E}[f(X_T)]-f(x)}{T} \]
\end{definition}

\begin{theorem}
Let \(X_t\) be the solution to \ref{sde}.  Then the generator \(A\) of \(X_t\) is the operator \(\alpha(x) \frac{\partial}{\partial x} +\frac{1}{2} \sigma^2(x) \frac{\partial ^2}{\partial x^2} \).  For the remainder of this paper the generator will be denoted \(A\).  

\end{theorem}

\begin{proof}
	By Ito's Lemma,
	\[\lim_{T \to 0} \frac{\mathbb{E}[f(X_T)]-f(x)}{T}\]
	\[=\lim_{T \to 0} \frac{\mathbb{E}\left[ \int_0 ^ T \left( \alpha(X_s) \frac{\partial f}{\partial x}(X_s) +\frac{1}{2} \sigma^2(X_s) \frac{\partial^2 f}{\partial x^2}(X_s) \right) ds + \int_0^T \sigma(X_s)dW_s \right]}{T} \]
	
	\[ = \lim_{T \to 0} \frac{ \int_0 ^ T \left( \mathbb{E}\left[\alpha(X_s) \frac{\partial f}{\partial x}(X_s) +\frac{1}{2} \sigma^2(X_s) \frac{\partial^2 f}{\partial x^2}(X_s) \right] \right) ds  }{T} \]
	\[ = \mathbb{E}\left[\lim_{T \to 0} \frac{ \int_0 ^ T \left( \alpha(X_s) \frac{\partial f}{\partial x}(X_s) +\frac{1}{2} \sigma^2(X_s) \frac{\partial^2 f}{\partial x^2}(X_s)  \right) ds }{T} \right]  \]
	\[ = \mathbb{E}\left[\alpha(x) \frac{\partial f}{\partial x} (x) +\frac{1}{2} \sigma^2(x) \frac{\partial^2 f}{\partial x^2}(x) \right] \]
	\[ = \alpha(x) \frac{\partial f}{\partial x} +\frac{1}{2} \sigma^2(x) \frac{\partial^2 f}{\partial x^2}\]
	
\end{proof}

\begin{theorem}\label{fk}
	Let \(X_t\) be the solution to \ref{sde} and define \(\tau=T-s\).  Then for any Borel measurable function \(h\),
	\(u(\tau, X_s)=\mathbb{E}[h(X_T)|\mathcal{F}_s]\) satisfies the partial differential equation
	\(\frac{\partial u}{\partial \tau} = Au \) with terminal condition \(u(0, x)=h(x)\)
	where \(A\) is the generator of \(X_t\),  and \(\mathcal{F}_t\) is the filtration generated by \(dW_t\).
	
\end{theorem}

\begin{proof}
	By the Markov property, there exists a function \(u\) such that \(u(T-s, X_s)=u(\tau, X_s)=\mathbb{E}[h(X_T)|\mathcal{F}_s]\). 
	\\
	\\
	By the definition of a generator,
	\[Au=\lim_{r \to 0} \frac{\mathbb{E}\left[ u(\tau, X_{s+r}) | \mathcal{F}_s \right] - u(\tau, X_s)}{r} \]
	
	\[ =  \lim_{r \to 0} \frac{\mathbb{E}\left[ \mathbb{E}\left[ h(X_{T+r}) | \mathcal{F}_{s+r} \right] \right] - u(\tau, X_s)}{r} \]
	
	\[ =  \lim_{r \to 0} \frac{ \mathbb{E}\left[ h(X_{T+r}) | \mathcal{F}_s \right] - u(\tau, X_s)}{r} \]
	
	\[ =  \lim_{r \to 0} \frac{ u(\tau+r, X_s) - u(\tau, X_s)}{r} \]
	
	\[= \frac{\partial u}{\partial \tau}\]
	

\end{proof}


\section{Backward Kolmogorov Equation}

The backward Kolmogorov equation is an application of theorem \ref{fk}.  It describes, for a fixed point at time \(T\), how the conditional density of \(X_t\) evolves.  I write this conditional density as \(p(s, T, x, y)\) where \(s\) is the ``current'' time, \(x\) is the ``current'' value of \(X_t\), \(T\) is the ``terminal'' time, and \(y\) is the ``dummy'' variable for integration.  The expectation \(\mathbb{E}[g(X_T)|\mathcal{F}_s]\) can thus be written as \(\int_\mathbb{R} g(y) p(s, T, x, y) dy \).   
\\
\\
\begin{theorem}\label{bk}
	Let \(X_t\) be defined as in \ref{sde}. If it exists, and fixing \(T\) and \(y\), the transition density \(p(s, T, x, y)\) of \(X_t\) is a solution to the following partial differential equation:
	\[\frac{\partial p}{\partial s} + Ap=0 \]
	With terminal condition \(p(T, T, x, y)=\delta(x-y)\).  \(A\) operates on \(x\).
\end{theorem}

\begin{proof}
	Let \(u_\delta (\tau, x)= \mathbb{E}\left[\delta(X_T-y)  | \mathcal{F}_s\right]\).
	By basic properties of delta functions, 
	\[u_\delta(\tau, x)=\mathbb{E}\left[\delta(X_T-y) | \mathcal{F}_s \right]=\int_\mathbb{R} \delta(\hat{y}-y) p(s, T, x, \hat{y}) d\hat{y}=p(s, T, x, y)\]
	From theorem \ref{fk}, \(\frac{\partial u_\delta}{\partial \tau}=Au_\delta\).  Since \(\tau=T-s\), \(\frac{\partial u_\delta}{\partial s} = \frac{\partial u_\delta}{\partial \tau} \frac{\partial \tau}{\partial s}=-\frac{\partial u_\delta}{\partial \tau} \).  Putting it all together, 
	
	\[\frac{\partial p}{\partial s} + Ap=0 \]
	
\end{proof}

\section{Forward Kolmogorov Equation}

The forward Kolmogorov equation describes, for a fixed point of time \(s\) and \(x\), how the conditional density of \(X_t\) evolves.  In many applications this is a more useful formulation.  For many applications we know \(s\) and \(x\), and are attempting to understand the potential outcomes in the future.  

\begin{theorem}\label{fk}
	Let \(X_t\) be defined as in \ref{sde}.  If it exists, and fixing \(s\) and \(x\), the transition density \(p(s, T, x, y)\) of \(X_t\) is a solution to the following partial differential equation:
	\[\frac{\partial p}{\partial T} + A^*p=0 \]
	With initial condition \(p(s, s, x, y)=\delta(x-y)\), where \(A^*f\) is defined as: \[-\frac{\partial \left(\alpha(y) f\right)}{\partial y} + \frac{1}{2}\frac{\partial^2 \left(\sigma(y) f\right) } {\partial y^2}   \]
	
	\(A^*\) operates on \(y\).
\end{theorem}
\begin{proof}
	The adjoint of \(A\) is defined as an operator \(\hat{A}\) such that:
	\[\int_\mathbb{R} f(z) A g(z) dz = \int_\mathbb{R} g(z) \hat{A} f(z)\,\,\forall f, g \in \mathcal{D}\]
	For our purposes, \(\mathcal{D}\) is the set of density functions.  Since densities integrate to one, these functions satisfy \(\lim_{z \to \infty } D(z) =0 \) and \(\lim_{z \to -\infty} D(z)=0\,\,\forall D \in \mathcal{D}\).
	\\
	\\
	To derive \(\hat{A}\),  
	\[\int_\mathbb{R} f(z) A g(z) dz  = \int_\mathbb{R}  f(z)  \alpha(z) \frac{\partial g}{\partial z} dz+\frac{1}{2} \int_\mathbb{R} f(z)\sigma^2(z) \frac{\partial^2 g}{\partial z^2} dz \]
	
	\[= - \int_\mathbb{R}  \frac{\partial \left( f(z)  \alpha(z) \right)}{\partial z} g(z) dz -\frac{1}{2} \int_\mathbb{R} \frac{\partial \left(f(z)\sigma^2(z)\right) }{\partial z} \frac{\partial g}{\partial z} dz   \]
	
	\[ = - \int_\mathbb{R}  \frac{\partial \left( f(z)  \alpha(z) \right)}{\partial z} g(z) dz +\frac{1}{2} \int_\mathbb{R} \frac{\partial ^2 \left(f(z)\sigma^2(z) \right)}{\partial z^2} g(z) dz   \]
	
	
	\[ \implies \hat{A}= -\frac{\partial \left(\alpha(y) f\right)}{\partial y} + \frac{1}{2}\frac{\partial^2 \left(\sigma(y) f\right) } {\partial y^2} =A^* \]
	
	Armed with the adjoint, I now proceed to directly compute the dynamics of the density.  By Dynkan's formula,
	
	\[ \mathbb{E}[h(X_T)|\mathcal{F}_s]= h(x)+\mathbb{E}\left[\int_s^T A h ds \right]  \]
	Substituting for the density, 
	\[\int_\mathbb{R} h(y) p(s, T, x, y) dy = h(x)+ \int_s^T  \int_\mathbb{R}p(s, v, x, y) A h(y)   dy dv   \]
	
	Taking the derivative with respect to \(T\) of both sides,
	\[\int_\mathbb{R} h(y) \frac{\partial p(s, T, x, y)}{\partial T} dy = \int_\mathbb{R}  p(s, T, x, y) A h(y) dy   \]
	
	Using the adjoint,
	\[\int_\mathbb{R} h(y) \frac{\partial p(s, T, x, y)}{\partial T} dy= \int_\mathbb{R}  h(y) A^* p(s, T, x, y) dy  \]
	Where the adjoint operates on the \(y\) variable.  The only way this equation holds for all \(h(y)\) is if 
	
	\[\frac{\partial p(s, T, x, y)}{\partial T} = A^* p(s, T, x, y)  \]
	
	
\end{proof}

\section{Examples}

\subsection{Brownian Motion}
\subsubsection{Backward equation}
When \(dX_t=dW_t\), the Backward equation becomes

\[\frac{\partial p} {\partial s} + \frac{1}{2} \frac{\partial^2 p}{\partial x^2} =0\]

The conditional density of a Brownian Motion is

\[p_{bm}(s, T, x, y)=\frac{1}{\sqrt{2\pi} \sqrt{T-s}} e^{-\frac{(y-x)^2}{2(T-s)}} \]

Taking the first derivative with respect to \(s\):

\[\frac{\partial p_{bm}(s, T, x, y)}{\partial s}=  \frac{1}{2}p_{bm}(s, T, x, y)\left(\frac{1}{T-s}-\frac{(y-x)^2}{(T-s)^2}\right)  \]

Taking the second derivative with respect to \(x\):

\[\frac{\partial^2 p_{bm}(s, T, x, y)}{\partial x^2}=  p_{bm}(s, T, x, y)\left(\frac{(y-x)^2}{(T-s)^2}-\frac{1}{T-s}\right)  \]

Combining,
\[\frac{\partial p} {\partial s} + \frac{1}{2} \frac{\partial^2 p}{\partial x^2} =0\]
\subsubsection{Forward equation}
The Forward equation becomes

\[\frac{\partial p} {\partial T} -\frac{1}{2} \frac{\partial^2 p}{\partial y^2} =0\]

Taking the first derivative with respect to \(T\):

\[\frac{\partial p_{bm}(s, T, x, y)}{\partial T}=  \frac{1}{2}p_{bm}(s, T, x, y)\left(\frac{(y-x)^2}{(T-s)^2}-\frac{1}{T-s}\right)  \]

Taking the second derivative with respect to \(y\):

\[\frac{\partial^2 p_{bm}(s, T, x, y)}{\partial y^2}=  p_{bm}(s, T, x, y)\left(\frac{(y-x)^2}{(T-s)^2}-\frac{1}{T-s}\right)  \]

Combining, 

\[\frac{\partial p} {\partial T} -\frac{1}{2} \frac{\partial^2 p}{\partial y^2} =0\]

\subsection{Geometric Brownian Motion}
\subsubsection{Backward equation}
When \(dX_t=\alpha X_t dt+\sigma X_t dW_t\), the Backward equation becomes

\[\frac{\partial p} {\partial s} +\alpha x \frac{\partial p}{\partial x} + \frac{1}{2} \sigma^2 x^2 \frac{\partial^2 p}{\partial x^2} =0\]

The conditional density of a Geometric Brownian Motion is

\[p_{gbm}(s, T, x, y)=\frac{1}{\sigma y\sqrt{2\pi} \sqrt{T-s} } e^{-\frac{\left( \log\left(\frac{y}{x}\right) -\left(\alpha-\frac{\sigma^2}{2}\right)(T-s)\right)^2}{2\sigma^2(T-s)}} \]

To simplify notation, from here on \(v\) denotes \(\frac{ \log\left(\frac{y}{x}\right) -\left(\alpha-\frac{\sigma^2}{2}\right)(T-s)}{T-s}\).
\\
\\
Taking the first derivative with respect to \(s\):


\begin{multline*}
\frac{\partial p_{gbm}(s, T, x, y)}{\partial s}  = \\ p_{gbm}(s, T, x, y) \frac{1}{2(T-s)} + p_{gbm}(s, T, x, y) \frac{v}{2}   - \\p_{gbm}(s, T, x, y) \alpha\frac{v}{\sigma^2}  -p_{gbm}(s, T, x, y)\frac{v^2}{2\sigma^2}  
\end{multline*}


Taking the first derivative with respect to \(x\):

\[\frac{\partial p_{gbm}(s, T, x, y)}{\partial x}=  p_{gbm}(s, T, x, y)\left(\frac{v }{\sigma^2 x}\right)  \]

Taking the second derivative with respect to \(x\):

\[\frac{\partial^2 p_{gbm}(s, T, x, y)}{\partial x^2}=  p_{gbm}(s, T, x, y)*\left(  \left(\frac{v }{\sigma^2 x}\right)^2 -\frac{1}{\sigma^2 x^2 (T-s)} - \frac{v }{\sigma^2 x^2}   \right)\]

Applying the coefficients,

\[\alpha x \frac{\partial p}{\partial x}=p \alpha \frac{v }{\sigma^2 }\]

\[\frac{1}{2} \sigma^2 x^2 \frac{\partial^2 p}{\partial x^2}\\=p\left( \frac{v^2 }{2\sigma^2}- \frac{1}{ 2(T-s)} - \frac{v }{ 2}      \right)\]

Putting it all together,


\begin{multline*}
\frac{\partial p} {\partial s} +\alpha x \frac{\partial p}{\partial x} + \frac{1}{2} \sigma^2 x^2 \frac{\partial^2 p}{\partial x^2}= \\ p\frac{1}{2(T-s)} + p\frac{v}{2}   - p \alpha \frac{v}{\sigma^2}  -p\frac{v^2}{2\sigma^2}  +  p \alpha \frac{v }{\sigma^2} + p\frac{v^2 }{2\sigma^2 }- p\frac{1}{ 2(T-s)} -p \frac{v }{ 2} =0
\end{multline*}

\subsubsection{Forward equation}
The Forward equation becomes

\[\frac{\partial p} {\partial T}+\frac{\partial \left(\alpha y p\right)}{\partial y}-\frac{1}{2} \frac{\partial^2 \left(\sigma^2 y^2 p\right)}{\partial y^2} =0 \]

Taking the first derivative with respect to \(T\):

\[\frac{\partial p_{gbm}(s, T, x, y)}{\partial T}=  -\frac{\partial p_{gbm}(s, T, x, y)}{\partial s} = p \alpha \frac{v}{\sigma^2} + p\frac{v^2}{2\sigma^2}- p\frac{1}{2(T-s)}- p\frac{v}{2}    \]

Where again \(v=\frac{ \log\left(\frac{y}{x}\right) -\left(\alpha-\frac{\sigma^2}{2}\right)(T-s)}{T-s}\).
\\
\\
Taking the first derivative with respect to \(y\):

\[\frac{\partial \left(\alpha y p\right)}{\partial y}= \frac{\partial \left(\frac{\alpha}{\sigma \sqrt{2\pi} \sqrt{T-s} } e^{-\frac{(T-s)v^2}{2\sigma^2}}\right)}{\partial y} \]

\[=-p\alpha\frac{v }{\sigma^2}\]

Taking the second derivative with respect to \(y\):

\[\frac{\partial^2 \left(\sigma^2 y^2 p\right)}{\partial y^2}=\frac{\partial^2 \left(\frac{\sigma y }{\sqrt{2\pi} \sqrt{T-s} } e^{-\frac{(T-s)v^2}{2\sigma^2}}\right)}{\partial y^2} \]

\[=p\frac{v^2}{\sigma^2} -p v-p \frac{1}{T-s} \]

Combining, 

\begin{multline*} \frac{\partial p} {\partial T}+\frac{\partial \left(\alpha y p\right)}{\partial y}-\frac{1}{2} \frac{\partial^2 \left(\sigma^2 y^2 p\right)}{\partial y^2}= \\p \alpha \frac{v}{\sigma^2} + p\frac{v^2}{2\sigma^2}- p\frac{1}{2(T-s)}- p\frac{v}{2}- p\alpha\frac{v }{\sigma^2} +  p \frac{1}{2(T-s)} +p \frac{v}{2}- p\frac{v^2}{2\sigma^2}=0\end{multline*}






\end{document}